# Курс Байесовские методы анализа данных, ФТиАД 2019

### Где и когда
Занятия проходят по понедельникам, 18:10 — 21:00, ауд. R505 (Покровский бульвар, 11).

### Ссылки
Чат в telegram: https://t.me/joinchat/DEBCqhcF1VFBC5qnQGfdVg

[Anytask курса](https://anytask.org/course/556) (инвайт в чате)

### Правила выставления оценок
В курсе предусмотрено несколько форм контроля знания:
* Домашние работы (на Python/NumPy и теоретические)
* Экзамен

Итоговая оценка вычисляется на основе оценки за работу в семестре и оценки за экзамен:

O<sub>итоговая</sub> = 0.7 * О<sub>накопленная</sub> + 0.3 * О<sub>экз</sub>

O<sub>накопленная</sub> = О<sub>домашние задания</sub>

Оценка за домашнюю работу вычисляется как среднее по домашним заданиям.

Накопленная, экзаменационная и итоговая оценки округляются арифметически.

### Экзамен
[Вопросы к экзамену (прошлогодняя версия)](https://github.com/ftad/BM2018/blob/master/materials/BMMO_exam.pdf)

Экзамен письменный, экзаменационная работа состоит из теоретических вопросов из списка вопросов и задач. Продолжительность написания: 1 час 30 минут. На экзамене нельзя ничем пользоваться.

Во время экзамена на проекторе или доске будут отображаться некоторые формулы (список в начале списка вопросов).

### Правила сдачи домашних заданий

Дедлайн по заданиям — начало следующего занятия (жесткий).

Два раза за курс дедлайн можно просрочить, в этом случае за каждый день просрочки будет начисляться штраф -1 балл.

При обнаружении плагиата оценки за домашнее задание обнуляются всем задействованным в списывании студентам. Это очень строгое правило!

### Материалы занятий
__На канале ФКН на Youtube доступны [видео](https://www.youtube.com/watch?v=Ejsr3S79gcQ&list=PLEqoHzpnmTfCiJpMPccTWXD9DB4ERQkyw) лекций Ветрова Д. П.__

__Занятие 1. Байесовские рассуждения__
* [Видео лекции](https://www.youtube.com/playlist?list=PLEqoHzpnmTfCiJpMPccTWXD9DB4ERQkyw)
* [Конспект лекции](https://drive.google.com/file/d/13Q58mRGh5uN8xyhMiTfoOXOYvxUKbvRY/view)
* [Еще один конспект](http://www.machinelearning.ru/wiki/images/8/8c/Lecture7_2012.pdf)
* [Задачи семинара](http://www.machinelearning.ru/wiki/images/1/18/S01_bayesian_reasoning_2016.pdf)

__Занятие 2. Сопряженные распределения и принцип наибольшей обоснованности__
* [Конспект-презентация про сопряженные распределения](http://www.machinelearning.ru/wiki/images/b/bd/BMMO11_5.pdf)
* [Конспект-презентация про обоснованность](http://www.machinelearning.ru/wiki/images/b/bd/BMMO11_5.pdf)
* [Мини-конспект про сопряженные распределения](https://drive.google.com/file/d/1g9cNLw85MchawKbSV7F0nUXyEi9m36sR/view)
* [Мини-конспект про обоснованность](https://drive.google.com/file/d/1l8fhZQ5V60wZaL9n_YlKNESW1y01PtX2/view?usp=sharing)
* [Презентация с распределениями](https://github.com/ftad/BM2018/blob/master/materials/distributions.pdf) 

__Занятие 3. Экспоненциальный класс распределений и матричное дифференцирование__
* [Конспект о матричном дифференцировании](http://www.machinelearning.ru/wiki/images/1/16/S04_matrix_calculations.pdf)
* [Об экспоненциальном классе](https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf) от Michael I. Jordan
* [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) не очень доступно но полно

__Занятие 4. Метод релевантных векторов__
* [Конспект-презентация](http://www.machinelearning.ru/wiki/images/d/d0/BMMO11_7.pdf)
* [Презентация с экспериментами](http://www.machinelearning.ru/wiki/images/8/8d/BMML15_S06_show.pdf)

__Занятие 5. EM-алгоритм__
* [Презентация по EM-алгоритму](https://drive.google.com/file/d/1CFGIuArumNz-qjVdCQqlxSpRbgGG3Ij_/view?usp=sharing)

__Занятие 6. EM-алгоритм. Решение задач__
* [Презентация по задаче с восстановлением фото](https://github.com/ftad/BM2018/blob/master/homeworks/homework6_theory.pdf)
* [Байесовский метод главных компонент](http://www.machinelearning.ru/wiki/images/7/73/BMMO11_11.pdf)

__Занятие 7. EM-алгоритм. Вариационный вывод__
* [Конспект с примерами применения](http://www.machinelearning.ru/wiki/images/3/34/Variational_inference.pdf)
* Теория с пояснениями - 21 chapter, Murphy K.P. Machine Learning: A Probabilistic Perspective.

__Занятие 8. Байесовские нейронные сети__
* [Презентация по байесовским нейронным сетям](https://drive.google.com/file/d/1yO2IQjYhx1R39ZSOSbJG7V0knwI_X7YC/view?usp=sharing)
* [Презентация по разреживающему вариационному дропауту](https://drive.google.com/file/d/1ZHy_26SOTpSLrYSfuBDF4khvrYlRmc-U/view?usp=sharing)
* Статьи: [VarDrop & LRT](https://arxiv.org/pdf/1506.02557.pdf), [ARD for NNs](https://arxiv.org/pdf/1811.00596.pdf), [SparseVD](https://arxiv.org/pdf/1701.05369.pdf), [BinDrop as BayesianNN](https://arxiv.org/pdf/1512.05287.pdf)

__Занятие 9. Вариационные автокодировщики__
* [Презентация](https://drive.google.com/file/d/1NqtMy7uMti9Xrsck9WIqvv8o3PWP1jS4/view?usp=sharing)

__Занятие 10. Методы Монте-Карло с марковскими цепями__
* [Конспект](http://www.machinelearning.ru/wiki/images/6/6b/BMMO11_10.pdf)
* [Подробный туториал на английском](https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf)

__Занятие 11. Latent Dirichlet Allocation__
* [Конспект лекции Ветрова](http://www.machinelearning.ru/wiki/images/8/82/BMMO11_14.pdf)
* Теория - 23 chapter, Murphy K.P. Machine Learning: A Probabilistic Perspective.
* [Про тематическое моделирование](http://www.machinelearning.ru/wiki/index.php?title=%D0%A2%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5)

__Занятие 12. Гауссовские процессы__
* [Презентация Е. Бурнаева](https://drive.google.com/file/d/1yhSOkV2TNCSrjbrNMUtYerXXZY1dQpo4/view?usp=sharing)
* [Презентация М. Филиппоне](https://drive.google.com/file/d/0B2zoFVYw1rN3SDJ0OU1nNVRxVWc/view?usp=sharing)

__Занятие 12. State-Space Model. Bayesian Structural Time Series__
* Теория - 18 chapter, Murphy K.P. Machine Learning: A Probabilistic Perspective.
* [Видео](https://www.youtube.com/watch?v=GTgZfCltMm8) о CausalImpact от создателя
* [Статья](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41854.pdf) о CausalImpact
* [Библиотека](https://google.github.io/CausalImpact/CausalImpact.html) CausalImpact на R (есть порт на python)

### Задания
* [Домашнее задание 1](https://github.com/ftad/BM2018/blob/master/homeworks/homework1.pdf). Дедлайн: 18:00 16.09.19.
* [Домашнее задание 2](https://github.com/ftad/BM2019/blob/master/materials/homework2%202019.pdf). Дедлайн: 18:00 23.09.19.
* [Домашнее задание 3](https://github.com/ftad/BM2019/blob/master/materials/homework3%202019.pdf). Дедлайн: 18:00 30.09.19.


### Полезные материалы
Книги:
* Barber D. [Bayesian Reasoning and Machine Learning.](http://www0.cs.ucl.ac.uk/staff/d.barber/brml/) Cambridge University Press, 2012.
* Murphy K.P. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.
* Bishop C.M. [Pattern Recognition and Machine Learning.](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) Springer, 2006. 
* Mackay D.J.C. [Information Theory, Inference, and Learning Algorithms.](http://www.inference.phy.cam.ac.uk/mackay/itila/book.html) Cambridge University Press, 2003. 
* Tipping M. [Sparse Bayesian Learning.](http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf) Journal of Machine Learning Research, 1, 2001, pp. 211-244. 
* Шумский С.А. [Байесова регуляризация обучения.](http://www.niisi.ru/iont/ni/Library/School-2002/Shumsky-2002.pdf) В сб. Лекции по нейроинформатике, часть 2, 2002.

Простые и удобные [заметки](http://cs.nyu.edu/~roweis/notes.html) по матричным вычислениям и свойствам гауссовских распределений.

[Памятка](http://statistics.zone/) по теории вероятностей.
